{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6c1b4d6",
   "metadata": {},
   "source": [
    "# Zadanie dnia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "421c5c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Title              Date  \\\n",
      "7                     Sem. Topologia i T. Mnogości  2023-12-20 16:15   \n",
      "42      Seminarium badawcze „Systemy Inteligentne”  2023-11-24 16:15   \n",
      "29                             Sem. Topologia Alg.  2023-12-06 10:30   \n",
      "2   North Atlantic Noncommutative Geometry Seminar  2024-01-24 17:15   \n",
      "14                                         Sem. RP  2023-12-14 12:15   \n",
      "\n",
      "                                                 href  \n",
      "7   https://www.mimuw.edu.pl/seminaria/sem-topolog...  \n",
      "42  https://www.mimuw.edu.pl/seminaria/seminarium-...  \n",
      "29  https://www.mimuw.edu.pl/seminaria/sem-topolog...  \n",
      "2   https://www.mimuw.edu.pl/seminaria/north-atlan...  \n",
      "14          https://www.mimuw.edu.pl/seminaria/sem-rp  \n"
     ]
    }
   ],
   "source": [
    "import requests, csv\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "data = requests.get(\"https://www.mimuw.edu.pl\")\n",
    "soup = BeautifulSoup(data.text, 'html.parser')\n",
    "\n",
    "events = []\n",
    "\n",
    "for event in soup.find_all('p', attrs={\"date\"}):\n",
    "\tparent = event.find_parent()\n",
    "\takt = dict()\n",
    "\takt[\"Date\"] = event.text\n",
    "\takt[\"Title\"] = parent.find_all(('a'))[-1].text\n",
    "\takt[\"href\"] = \"https://www.mimuw.edu.pl\" + parent.find_all(('a'))[-1][\"href\"]\n",
    "\tevents.append(akt)\n",
    " \n",
    "csv_file_name = 'events.csv'\n",
    "\n",
    "field_names = ['Title', 'Date', 'href']\n",
    "\n",
    "with open(csv_file_name, 'w', newline='') as csv_file:\n",
    "    csv_writer = csv.DictWriter(csv_file, fieldnames=field_names)\n",
    "    csv_writer.writeheader()\n",
    "    csv_writer.writerows(events)\n",
    "    \n",
    "df = pd.read_csv(csv_file_name)\n",
    "\n",
    "sample = df.sample(5)  \n",
    "print(sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd186b27",
   "metadata": {},
   "source": [
    "* Należy pobrać z wykorzystaniem pakietów requests, Beautiful Soup wszystkie wydarzenia zamieszczone w harmonogramie na stronie https://www.mimuw.edu.pl/. \n",
    "* Proszę napisać scraper w taki sposób, aby w efekcie działania zapisywał informacje jako lista słowników, których kluczami są tytuł, url do wydarzenia oraz jego data i godzina.\n",
    "* Otrzymane dane proszę zapisać w formacie csv/xlsx - pomocne może być wykorzystanie ramki danych (pakiet pandas) i metody from_dict.\n",
    "* Na koniec - prosze o wyświetlenie pięciu losowych wydarzeń z listy pobranych (tytuł, url, data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e58cc39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2e04e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>href</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Unveiling Microbial Dark Matter: leveraging co...</td>\n",
       "      <td>https://www.mimuw.edu.pl/aktualnosci/seminaria...</td>\n",
       "      <td>2023-12-06 10:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>North Atlantic Noncommutative Geometry Seminar</td>\n",
       "      <td>https://www.mimuw.edu.pl/aktualnosci/seminaria...</td>\n",
       "      <td>2024-01-24 17:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Coercive inequalities for Gibbs measures</td>\n",
       "      <td>https://www.mimuw.edu.pl/aktualnosci/seminaria...</td>\n",
       "      <td>2023-12-14 12:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Magdalena Wiertel</td>\n",
       "      <td>https://www.mimuw.edu.pl/doktoraty/magdalena-w...</td>\n",
       "      <td>2023-11-16 16:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Foam evaluation and GL(N)-equivariant Khovanov...</td>\n",
       "      <td>https://www.mimuw.edu.pl/aktualnosci/seminaria...</td>\n",
       "      <td>2023-11-29 10:30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "22  Unveiling Microbial Dark Matter: leveraging co...   \n",
       "2      North Atlantic Noncommutative Geometry Seminar   \n",
       "7           Coercive inequalities for Gibbs measures    \n",
       "60                                  Magdalena Wiertel   \n",
       "33  Foam evaluation and GL(N)-equivariant Khovanov...   \n",
       "\n",
       "                                                 href              date  \n",
       "22  https://www.mimuw.edu.pl/aktualnosci/seminaria...  2023-12-06 10:15  \n",
       "2   https://www.mimuw.edu.pl/aktualnosci/seminaria...  2024-01-24 17:15  \n",
       "7   https://www.mimuw.edu.pl/aktualnosci/seminaria...  2023-12-14 12:15  \n",
       "60  https://www.mimuw.edu.pl/doktoraty/magdalena-w...  2023-11-16 16:15  \n",
       "33  https://www.mimuw.edu.pl/aktualnosci/seminaria...  2023-11-29 10:30  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fe6b588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from random import randint\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "\n",
    "#~ Nalezy zmienic API 06.04\n",
    "#~ https://app.scrapingbee.com/dashboard\n",
    "\n",
    "SCRAPEOPS_API_KEY = \"a863a8aa-6696-474f-8bc9-41c9f752a10a\"\n",
    "SCRAPINGBEE_API_KEY = \"HPWIYC12R40AJFXTPHUUTOFNMXMIEBY7YH3YQ63PD8RAR0N1N34UX7N6Z3JIQZBITDUZMJTEEW147TH7\"\n",
    "BASE_PATH = \"Items\"\n",
    "\n",
    "def get_headers_list():\n",
    "  response = requests.get('http://headers.scrapeops.io/v1/browser-headers?api_key=' + SCRAPEOPS_API_KEY)\n",
    "  json_response = response.json()\n",
    "  return json_response.get('result', [])\n",
    "\n",
    "def get_random_header(header_list):\n",
    "  random_index = randint(0, len(header_list) - 1)\n",
    "  return header_list[random_index]\n",
    "\n",
    "def scrapeWebsiteIntoJSON(id, filename = \"_tmp\", directory = \"_tmp\"):\n",
    "    #* Variables\n",
    "    url = \"http://steamcommunity.com/market/itemordershistogram?country=PL&language=polish&currency=6&item_nameid=\" + str(id) + \"&two_factor=0\"\n",
    "    filename = filename + \".json\"\n",
    "    \n",
    "    #* Security:\n",
    "    header_list = get_headers_list()\n",
    "    \n",
    "    proxies = \\\n",
    "    {\n",
    "      \"http\": \"http://\" + SCRAPINGBEE_API_KEY + \":render_js=False&premium_proxy=True@proxy.scrapingbee.com:8886\",\n",
    "      \"https\": \"https://\" + SCRAPINGBEE_API_KEY + \":render_js=False&premium_proxy=True@proxy.scrapingbee.com:8887\"\n",
    "    }\n",
    "    \n",
    "    #* Get source code\n",
    "    logging.captureWarnings(True)   # hide warnings, because there were a lot XDDD insecure stuff bla bla\n",
    "    request = requests.get(url=url, proxies=proxies, headers=get_random_header(header_list), verify=False)\n",
    "    pageSourceCode = request.text\n",
    "\n",
    "    #* Create folder\n",
    "    if not os.path.exists(os.path.join(BASE_PATH, directory)):\n",
    "        os.makedirs(os.path.join(BASE_PATH, directory))\n",
    "\n",
    "    #* Save into a file\n",
    "    f = open(os.path.join(BASE_PATH, directory, filename), 'w')\n",
    "    f.write(pageSourceCode)\n",
    "    f.close()\n",
    "    \n",
    "def getBuyOrderData(filename = \"_tmp\", directory = \"_tmp\"):\n",
    "    #* Variables\n",
    "    filename = filename + \".json\"\n",
    "    \n",
    "    #* Create folder\n",
    "    if not os.path.exists(os.path.join(BASE_PATH, directory)):\n",
    "        os.makedirs(os.path.join(BASE_PATH, directory))\n",
    "    \n",
    "    #* Open file and extract \"buy_order_graph\"\n",
    "    f = open(os.path.join(BASE_PATH, directory, filename))\n",
    "    data = json.load(f)\n",
    "    buyOrderGraph = data[\"buy_order_graph\"]\n",
    "\n",
    "    #* Remove useless data\n",
    "    for i in range(0, len(buyOrderGraph)):\n",
    "        buyOrderGraph[i] = buyOrderGraph[i][:-1]\n",
    "        \n",
    "    return buyOrderGraph\n",
    "    \n",
    "def getSellOrderData(filename = \"_tmp\", directory = \"_tmp\"):\n",
    "    #* Variables\n",
    "    filename = filename + \".json\"\n",
    "    \n",
    "    #* Create folder\n",
    "    if not os.path.exists(os.path.join(BASE_PATH, directory)):\n",
    "        os.makedirs(os.path.join(BASE_PATH, directory))\n",
    "    \n",
    "    #* Open file and extract \"sell_order_graph\"\n",
    "    f = open(os.path.join(BASE_PATH, directory, filename))\n",
    "    data = json.load(f)\n",
    "    sellOrderGraph = data[\"sell_order_graph\"]\n",
    "\n",
    "    #* Remove useless data\n",
    "    for i in range(0, len(sellOrderGraph)):\n",
    "        sellOrderGraph[i] = sellOrderGraph[i][:-1]\n",
    "        \n",
    "    return sellOrderGraph\n",
    "\n",
    "def undoPrefixSum(data):\n",
    "    cnt = 0\n",
    "    for i in range(len(data)):\n",
    "        tmp = data[i][1]\n",
    "        data[i][1] -= cnt\n",
    "        cnt = tmp\n",
    "        \n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7bd102",
   "metadata": {},
   "source": [
    "Należy wybrać dowolną stronę internetową (zgodnie z zainteresowaniami), która zezwala na pobieranie danych. Następnie proszę dokonać pobierania wybranych przez Państwa treści. Można spróbować wykorzystac Selenium, chociaż requests + Beautiful Soup są wystarczające. Uzasadnić do czego takie dane mogą się Państwu przydać.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
